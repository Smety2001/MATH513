---
title: "MATH513 Practical"
subtitle: "Using Twitter data to gain business advantage in the electric and fuel car industry"
author: |
    | \textbf{Group:} 36
    | \textbf{Members:} 10649798, 10654115, 10775412, 10777441
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: beamer_presentation
header-includes:
- \setbeamertemplate{navigation symbols}{}
- \setbeamertemplate{footline}[page number]
- \newcommand\Fontvi{\fontsize{6}{7.2}\selectfont}
- \newcommand\Fontmid{\fontsize{11}{7.2}\selectfont}
---


```{r setup, include=FALSE}
#setup chunk options
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE,
	results = FALSE
)

#load libraries
library(rtweet)
library(ggplot2)
library(dplyr)
library(ggthemes)
library(jsonlite)
library(tidytext)
library(wordcloud)
library(tidyr)
library(httpuv)
library(stringr)
library(readr)

#hashtags / words to search
search <- c("#ev", "#cars")

#labels for groups
groupLabels <- c("Electric", "Fuel")

########## GENERATE DATA.JSON ##########
#search all tweets containing the previously defined hashtags / words in the past 7 days
#('free' Twitter API limit is 7 days)
tweets <- search_tweets(q = paste(search, collapse = " OR "), n = 50000, include_rts = TRUE, retryonratelimit = TRUE, since = Sys.Date()-7, until = Sys.Date(), lang = "en")

#create column group to distinguish where the data came from
tweets$group <- "Total"

i <- 0
#for each hashtag / word
for (x in search){
  i <- i+1   
  #search the tweets containing the hashtag / word in the past 7 days
  temp <- search_tweets(q = x, n = 50000, include_rts = TRUE, retryonratelimit = TRUE, since = Sys.Date()-7, until = Sys.Date(), lang = "en")
    
  #create column group to distinguish where the data came from
  temp$group <- groupLabels[i]
    
  #combine the tweets
  tweets <- rbind(tweets, temp)
}

#change empty locations into NAs
tweets$location[tweets$location==""] <- NA

#calculate latitude and longitude of tweets
tweets <- lat_lng(tweets)

#save data into a JSON file (also deletes empty columns)
tweets %>% toJSON() %>% write_lines("Data.json")
########################################

#load data from a JSON file
tw <- stream_in(file("Data.json"))

#create data frames for each group
tweets <- tw[tw$group == "Total", ]
search1 <- tw[tw$group == groupLabels[1], ]
search2 <- tw[tw$group == groupLabels[2], ]

```


## Introduction
  * The innovation of electric cars over the years seems to gain more market recognition globally.
  
  * There are so many advantages of electric cars as they try to curb environmental deterioration.
  
  * Fuel Cars manufacturer seems to be overwhelmed with the fast pace of Electric Cars growth in the past few years.
  
  * This presentation aims to compare and contrast the business insight of electric cars and fuel cars. We shall collect and analyse data from twitter to gain insight on the best marketing strategies to be deployed.


## Methodology
  * This presentation was created using R markdown in a RStudio IDE, using R programming language.
      
  * The process began by searching tweets about electric and fuel cars, using the twitter API. However, as the 'free' twitter API is limited to only getting the data from the past 7 days, the data were far from representing the whole customer base.

  * Useful customer data was then extracted from the tweets, by making use of some basic R data manipulation techniques.
  
  * The data was then visualized by creating graphs, to make the understanding of the data easier
  
  * All the packages and tools that were used in the process are documented at the end of the presentation.


## Tweet frequency for each topic
```{r frequency}
#plot the time series grouped by "group", showing the frequency for each hashtag / word as well as total
ts_plot(data = group_by(tw, group), by = "1 hours", trim = 1) +
  scale_color_manual(values=c("#00BFC4", "#F8766D", "Black")) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold")) +
  labs(
    color = "Topic",
    x = "Datetime",
    y = "Number of tweets",
    title = "Frequency of tweets for each topic",
    subtitle = "Tweet counts aggregated using 1-hour intervals",
    caption = "Source: Data collected from Twitter's REST API via rtweet")
```


## Location of users
```{r Ulocation}
#get locations from first data frame and count them
location1 <- search1 %>% mutate(location = recode(location,
                                        "UK" = "United Kingdom",
                                        "USA" = "United States of America",
                                        "United States" = "United States of America",
                                        "Asokoro, Abuja 🇳🇬" = "Asokoro, Abuja",
                                        "London" = "London, United Kingdom",
                                        "England, United Kingdom" = "United Kingdom",
                                        "London, UK" = "London, United Kingdom",
                                        "London, England" = "London, United Kingdom",
                                        "Texas" = "Texas, USA",
                                        "Florida" = "Florida, USA",
                                        "Washington" = "Washington, USA",
                                        "Washington, D.C." = "Washington, DC",
                                        "New Delhi" = "New Delhi, India",
                                        "Dubai" = "Dubai, United Arab Emirates",
                                        "Mumbai, Maharashtra" = "Mumbai, India",
                                        "Mumbai" = "Mumbai, India",
                                        "Toronto" = "Toronto, Ontario",
                                        "Toronto, ON" = "Toronto, Ontario",
                                        "Vancouver, BC (TSX.V: TNR)" = "Vancouver, British Columbia",
                                        "Vancouver" = "Vancouver, British Columbia",
                                        "Vancouver, BC, Canada" = "Vancouver, British Columbia",
                                        "New York, NY" = "New York, USA",
                                        "New York" = "New York, USA",
                                        "MOMBASA, KENYA" = "Mombasa, Kenya",
                                        "Florida USA" = "Florida, USA"
                                     )) %>%
  count(location, sort = TRUE) %>% mutate(location = reorder(location,n)) %>% na.omit()
location1$group <- groupLabels[1]


#get locations from second data frame and count them
location2 <- search2 %>% mutate(location = recode(location,
                                        "UK" = "United Kingdom",
                                        "USA" = "United States of America",
                                        "United States" = "United States of America",
                                        "Asokoro, Abuja 🇳🇬" = "Asokoro, Abuja",
                                        "London" = "London, United Kingdom",
                                        "England, United Kingdom" = "United Kingdom",
                                        "London, UK" = "London, United Kingdom",
                                        "London, England" = "London, United Kingdom",
                                        "Texas" = "Texas, USA",
                                        "Florida" = "Florida, USA",
                                        "Washington" = "Washington, USA",
                                        "Washington, D.C." = "Washington, DC",
                                        "New Delhi" = "New Delhi, India",
                                        "Dubai" = "Dubai, United Arab Emirates",
                                        "Mumbai, Maharashtra" = "Mumbai, India",
                                        "Mumbai" = "Mumbai, India",
                                        "Toronto" = "Toronto, Ontario",
                                        "Toronto, ON" = "Toronto, Ontario",
                                        "Vancouver, BC (TSX.V: TNR)" = "Vancouver, British Columbia",
                                        "Vancouver" = "Vancouver, British Columbia",
                                        "Vancouver, BC, Canada" = "Vancouver, British Columbia",
                                        "New York, NY" = "New York, USA",
                                        "New York" = "New York, USA",
                                        "MOMBASA, KENYA" = "Mombasa, Kenya",
                                        "Florida USA" = "Florida, USA"
                                     )) %>%
  count(location, sort = TRUE) %>% mutate(location = reorder(location,n)) %>% na.omit()
location2$group <- groupLabels[2]

#combine locations
location <- rbind(location1, location2)
location <- location[!(location$location == "TOTES NOT FINANCIAL ADVICE!" | location$location == "from the future" | location$location == "Global." | location$location == "Global" | location$location == "Instagram: pastexpirycom" | location$location == "Earth?"), ]

#plot top 10 locations
location %>%
  group_by(group) %>%
  slice(1:10) %>%
  ggplot(aes(x = reorder(location, n, sum), y = n, fill = group)) +
  scale_fill_manual(values=c("#00BFC4","#F8766D")) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold")) +
  geom_col(color = "black") +
  coord_flip() +
  labs(
    fill = "Topic",
    x = "Top Locations",
    y = "Frequency",
    title = "Most common locations of users",
    caption = "Source: Data collected from Twitter's REST API via rtweet")
```


## Where people are tweeting from
```{r Tlocation}
#get latitude and longitude data from first data frame
map1 <- data.frame(long = search1$lng, lat = search1$lat) %>%
  na.omit() %>% mutate(long_round = round(long, 2), lat_round = round(lat, 2)) %>% group_by(long_round, lat_round) %>%
  summarise(n = n()) %>%
  ungroup()
map1$group <- groupLabels[1]

#get latitude and longitude data from second data frame  
map2 <- data.frame(long = search2$lng, lat = search2$lat) %>%
  na.omit() %>% mutate(long_round = round(long, 2), lat_round = round(lat, 2)) %>% group_by(long_round, lat_round) %>%
  summarise(n = n()) %>%
  ungroup()
map2$group <- groupLabels[2]
  
map <- rbind(map1, map2)

#plot the map of tweets
map %>%
  group_by(group) %>%
  ggplot() +
  borders("world", colour = "gray85", fill = "gray80") +
  theme_map() + 
  theme(plot.title = element_text(face = "bold")) +
  geom_point(aes(long_round, lat_round, size = n, color = group), alpha = .5) + 
  scale_color_manual(values=c("#00BFC4", "#F8766D")) +
  coord_fixed() +
  labs(
    color = "Topic",
    title = "Locations of tweets for each topic",
    size = "Number of Tweets",
    caption = "Source: Data collected from Twitter's REST API via rtweet")
```


## Devices used
```{r devices}
#get sources from first data frame
source1 <- search1 %>% count(source, sort = TRUE) %>% mutate(source = reorder(source,n)) %>% na.omit()
source1$group <- groupLabels[1]

#get sources from second data frame
source2 <- search2 %>% count(source, sort = TRUE) %>% mutate(source = reorder(source,n)) %>% na.omit()
source2$group <- groupLabels[2]

#combine the sources
source <- rbind(source1, source2)

#plot the devices used to tweet
source %>% 
  group_by(group) %>% 
  slice(1:10) %>%
  ggplot(aes(x = reorder(source, n, sum), y = n, fill = group)) +
  scale_fill_manual(values=c("#00BFC4","#F8766D")) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold")) +
  geom_col(color = "black") +
  coord_flip() + 
  labs(
    x = "Device", 
    y = "total",
    title = "Devices used to tweet about each topic",
    caption = "Source: Data collected from Twitter's REST API via rtweet") 
```


## Popular hashtags
```{r hashtags}
#extract hashtags from first data frame and count them
hashtags1 <- str_extract_all(search1$text, "#[a-zA-Z0-9_-ー\\.]+") %>% unlist() %>% tolower()
hashtags1 <- hashtags1[!str_detect(string = hashtags1, pattern = search[1])]
hashtags1 <- as.data.frame(hashtags1) %>% na.omit() %>% rename("hashtags" = "hashtags1") %>% count(hashtags, sort = TRUE) %>% mutate(hashtags = reorder(hashtags, n))
hashtags1$group <- groupLabels[1]

#extract hashtags from second data frame and count them
hashtags2 <- str_extract_all(search2$text, "#[a-zA-Z0-9_-ー\\.]+") %>% unlist() %>% tolower()
hashtags2 <- hashtags2[!str_detect(string = hashtags2, pattern = search[2])]
hashtags2 <- as.data.frame(hashtags2) %>% na.omit() %>% rename("hashtags" = "hashtags2") %>% count(hashtags, sort = TRUE) %>% mutate(hashtags = reorder(hashtags, n))
hashtags2$group <- groupLabels[2]

#combine hashtags
hashtags <- rbind(hashtags1, hashtags2)

#plot top 10 popular hastags alongside the ones you used
hashtags %>%
  group_by(group) %>%
  slice(1:10) %>%
  ggplot(mapping = aes(x = reorder(hashtags, n, sum), y = n, fill = group)) +
  scale_fill_manual(values=c("#00BFC4","#F8766D")) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold")) +
  geom_col(color = "black") +
  coord_flip() +
  labs(
    fill = "Topic",
    x = "Hashtag",
    y = "Count",
    title = "Most popular hashtags from each topic",
    caption = "Source: Data collected from Twitter's REST API via rtweet")
```


## Most commonly used words
```{r words}
#load stop words
data("stop_words")

#remove urls, hashtags, mentions and foreign characters from first data frame 
strippedText1 <- gsub(pattern = "http.*|https.*|amp|#[A-Za-z0-9]+|@[A-Za-z0-9]+|[^A-Za-z ]", replacement = "", x = search1$text)

#remove stop words and get tweet numbers from first data frame
strippedText1 <- as.data.frame(strippedText1) %>%
  mutate(tweetnumber = row_number()) %>%
  unnest_tokens(word, strippedText1) %>%
  anti_join(stop_words)
strippedText1$group <- groupLabels[1]

#remove urls, hashtags, mentions and foreign characters from second data frame 
strippedText2 <- gsub(pattern = "http.*|https.*|amp|#[A-Za-z0-9]+|@[A-Za-z0-9]+|[^A-Za-z ]", replacement = "", x = search2$text)

#remove stop words and get tweet numbers from second data frame
strippedText2 <- as.data.frame(strippedText2) %>%
  mutate(tweetnumber = row_number()) %>%
  unnest_tokens(word, strippedText2) %>%
  anti_join(stop_words)
strippedText2$group <- groupLabels[2]

#combine stripped text
cleanText <- rbind(strippedText1, strippedText2)

#plot top 10 unique words used in tweets
cleanText %>%
  group_by(group) %>%
  count(word, sort = TRUE) %>%
  slice(1:10) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = reorder(word, n, sum), y = n, fill = group)) +
  scale_fill_manual(values=c("#00BFC4","#F8766D")) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold")) +
  geom_col(color = "black") +
  coord_flip() +
  labs(
    fill = "Topic",
    x = "Unique Words",
    y = "Frequency",
    title = "Most commonly used words in tweets for each topic",
    caption = "Source: Data collected from Twitter's REST API via rtweet")
```


## Wordclouds
```{r wordcloud}
#prepare wordcloud data from first data frame
wc1 <- strippedText1 %>% count(word, sort = TRUE) %>% mutate(freq = n / sum(n))

#prepare wordcloud data from second data frame
wc2 <- strippedText2 %>% count(word, sort = TRUE) %>% mutate(freq = n / sum(n))

#plot the two wordclouds next to each other
par(mfrow=c(1,2))

#plot wordcloud1
wordcloud(wc1$word, wc1$freq, min.freq = 3, 
          scale = c(4.5, 0.1), random.order = FALSE, random.color = FALSE, 
          colors = c("#00BFC4"), max.words = 50) %>%
  title(main = "Wordcloud for Electric cars", cex.main = 1, sub = "Source: Data collected from Twitter's REST API via rtweet")

#plot wordcloud2
wordcloud(wc2$word, wc2$freq, min.freq = 3, 
          scale = c(4.5, 0.1), random.order = FALSE, random.color = FALSE, 
          colors= c("#F8766D"), max.words = 50) %>%
  title(main = "Wordcloud for Fuel cars", cex.main = 1, sub = "Source: Data collected from Twitter's REST API via rtweet")


```


## Sentiment scores
```{r sentiment}
#calculate sentiment scores from first data frame
sentiment1 <- strippedText1 %>%
  inner_join(get_sentiments("bing")) %>%
  count(tweetnumber, sentiment) %>%
  spread(sentiment, n, fill = 0) %>% 
  mutate(score = positive - negative) %>% 
  mutate(group = groupLabels[1])

#calculate sentiment scores from second data frame
sentiment2 <- strippedText2 %>%
  inner_join(get_sentiments("bing")) %>%
  count(tweetnumber, sentiment) %>%
  spread(sentiment, n, fill = 0) %>% 
  mutate(score = positive - negative) %>% 
  mutate(group = groupLabels[2])

#combine sentiments
sentiment <- rbind(sentiment1, sentiment2)

#calculate mean
sentimentMean <- sentiment %>% 
  group_by(group) %>% 
  summarize(meanScore = mean(score)) 

#plot the sentiments
ggplot(sentiment, aes(x = score, fill = group)) +
  geom_bar(color = "black") +
  scale_fill_manual(values=c("#00BFC4","#F8766D")) +
  geom_vline(aes(xintercept = meanScore), data = sentimentMean) +
  geom_text(aes(x = meanScore, 
                y = Inf, 
                label = signif(meanScore, 3)), 
            vjust = 2, 
            data = sentimentMean) + 
  scale_x_continuous(breaks = -15:15, minor_breaks = NULL) +
  labs(
    fill = "Topic",
    x = "Sentiment Score" ,
    y = "Number of tweets",
    title = "Sentiment scores for each topic",
    caption = "Source: Data collected from Twitter's REST API via rtweet") +
  facet_grid(group ~ .) +
  theme(legend.position = "bottom")
```


## Statistical test
```{r test}
#prepare data
x <- sentiment %>% select (score, group)

#split data
electric <- x %>% filter(group == groupLabels[1])
fuel <- x %>% filter(group == groupLabels[2])

#test to see if t-test is suitable
eshapiro <- with(electric, shapiro.test(score))$p.value
fshapiro <- with(fuel, shapiro.test(score))$p.value

#t-test not suitable, wilcox instead
wtest <- wilcox.test(score ~ group, x)$p.value

#significant difference, check if x is less than y
lwtest <- wilcox.test(x = electric$score, y = fuel$score, alternative = "less")$p.value
```
  * To see whether there is a significant difference between the sentiment scores, we can conduct a statistical test.
  
  * To check whether we can use a t-test we firstly need to check if the data results produce normal, bell-shaped distribution. We can do that by conducting a normality test, such as Shapiro-Wilk test.
  
  * The test results of Shapiro-Wilk test are as follows:
      + Electric cars p-value: `r eshapiro`
      + Fuel cars p-value: `r fshapiro`
      
  * Since the p-value is under 0.05, a t-test wouldn't be appropriate. Therefore we will use Mann-Whitney-Wilcoxon test. which yielded the following results:
      + p-value: `r wtest`
      
  * As the p-value is under 0.05, we can say that there is a significant difference between the sentiment scores.


## Conclusion
  * The various outcomes of our data analysis, data visualization and sentiment analysis provides us with useful insight on how to further expand the business of electric cars globally.

  * The highest frequency of tweets occurred at 2PM UTC indicating the best time to reach our customers on the twitter platform.

  * The location of users from our data visualization provided us with useful information on how Electric cars manufacturers should expand its business in the United States of America and in the United Kingdom.


## Conclusion - continued
  * The electric car manufacturers could reach out to their customers mostly by using Twitter Web App.

  * The tweets which used popular hashtags and words gain more impressions, therefore very useful for digital marketing.

  * From the sentiment score, tweets about electric cars are more positive than fuel cars. This indicates that the electric cars are well accepted from English speaking countries.

  * Based on our analysis, we advise the manufacturers of fuel cars to fully integrate the technology and innovation of electric cars as positive sentiment scores favored electric cars.


## Citations
\Fontvi
R Core Team (2022). R: A language and environment for statistical computing. R Foundation
  for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/. (R version 4.2.1)
  
Kearney, M. W. (2019). rtweet: Collecting and analyzing Twitter data, Journal of Open Source
  Software, 4, 42. 1829. doi:10.21105/joss.01829 (R package version 0.7.0)

H. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016.

Wickham H, François R, Henry L, Müller K (2022). _dplyr: A Grammar of Data Manipulation_. R
  package version 1.0.10, <https://CRAN.R-project.org/package=dplyr>.
  
Arnold J (2021). _ggthemes: Extra Themes, Scales and Geoms for 'ggplot2'_. R package version
  4.2.4, <https://CRAN.R-project.org/package=ggthemes>.
  
Jeroen Ooms (2014). The jsonlite Package: A Practical and Consistent Mapping Between JSON
  Data and R Objects. arXiv:1403.2805 [stat.CO] URL https://arxiv.org/abs/1403.2805.
  
Silge J, Robinson D (2016). “tidytext: Text Mining and Analysis Using Tidy Data Principles
  in R.” _JOSS_, *1*(3). doi:10.21105/joss.00037 <https://doi.org/10.21105/joss.00037>,
  <http://dx.doi.org/10.21105/joss.00037>.
  
Fellows I (2018). _wordcloud: Word Clouds_. R package version 2.6,
  <https://CRAN.R-project.org/package=wordcloud>.
  
Wickham H, Girlich M (2022). _tidyr: Tidy Messy Data_. R package version 1.2.1,
  <https://CRAN.R-project.org/package=tidyr>.
  
Cheng J, Chang W (2022). _httpuv: HTTP and WebSocket Server Library_. R package version
  1.6.6, <https://CRAN.R-project.org/package=httpuv>.
  
Wickham H (2022). _stringr: Simple, Consistent Wrappers for Common String Operations_. R
  package version 1.4.1, <https://CRAN.R-project.org/package=stringr>.
  
Wickham H, Hester J, Bryan J (2022). _readr: Read Rectangular Text Data_. R package version
  2.1.3, <https://CRAN.R-project.org/package=readr>.